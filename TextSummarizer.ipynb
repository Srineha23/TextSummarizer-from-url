{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3DNhBkqS8DV",
        "outputId": "970f13b2-f04e-450b-e2c0-d6acb4191f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Q4bvRRqTLGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W30_Qtn8S9_X",
        "outputId": "a0f327df-e444-43bc-908c-369d585c3919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "import heapq\n",
        "#nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "B6r64IZwTEau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = urllib.request.urlopen(\"https://www.imperva.com/learn/application-security/web-scraping-attack/#:~:text=Web%20scraping%20is%20the%20process,replicate%20entire%20website%20content%20elsewhere.\").read()\n",
        "soup = bs.BeautifulSoup(source,'lxml')\n",
        "text = \"\""
      ],
      "metadata": {
        "id": "WC2YCEyhQOmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding all the text that are in the paragraph tag in html.\n",
        "#fetch all the text\n",
        "for paragraph in soup.find_all('p'):\n",
        "    text += paragraph.text\n",
        "#preprocessing the text require re library\n",
        "#removing all the number in the [] in the text\n",
        "text=re.sub(r'\\[[0-9]*\\]',' ',text)\n",
        "#remove extraspace\n",
        "text=re.sub(r\"\\s+\",\" \",text)\n",
        "clean_text=text.lower()\n",
        "#remove the non-word character\n",
        "clean_text=re.sub(r\"\\W\",\" \",clean_text)\n",
        "#remove all digits\n",
        "clean_text=re.sub(r\"\\d\",\" \",clean_text)\n",
        "#remove the extra space\n",
        "clean_text = re.sub(r\"\\s+\",\" \",clean_text)\n",
        "#tokenise the whole paragraph to sentences wwe require nltk library"
      ],
      "metadata": {
        "id": "yt4kiyKEQOyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(text)\n",
        "stop_words=nltk.corpus.stopwords.words(\"english\")\n",
        "#creating histogram where words-->keys and count-->values\n",
        "word2count={}\n",
        "for word in nltk.word_tokenize(clean_text):\n",
        "    if word not in stop_words:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word]=1\n",
        "        else:\n",
        "            word2count[word]+=1\n",
        "#converting counts to weights\n",
        "max_count = max(word2count.values())\n",
        "for key in word2count.keys():\n",
        "    word2count[key] = word2count[key]/max_count\n",
        "\n",
        "#create sentence score\n",
        "sent2score = {}\n",
        "for sentence in sentences:\n",
        "    for word in nltk.word_tokenize(sentence.lower()):\n",
        "        if word in word2count.keys():\n",
        "            if len(sentence.split(\" \")) < 25:\n",
        "                if sentence not in sent2score.keys():\n",
        "                    sent2score[sentence] = word2count[word]\n",
        "                else:\n",
        "                    sent2score[sentence] += word2count[word]\n",
        "#print the number of sentence you want to display\n",
        "#5 in parameter is the number of line you wanted to print\n",
        "best_sent = heapq.nlargest(5, sent2score, key=sent2score.get)"
      ],
      "metadata": {
        "id": "MBIAxoXKQYCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in best_sent:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shev_bieQYO3",
        "outputId": "bea1a384-a922-4afd-fd6e-773ef64b9cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Legitimate use cases include:Web scraping is also used for illegal purposes, including the undercutting of prices and the theft of copyrighted content.\n",
            "The scraper can then replicate entire website content elsewhere.Web scraping is used in a variety of digital businesses that rely on data harvesting.\n",
            "In price scraping, a perpetrator typically uses a botnet from which to launch scraper bots to inspect competing business databases.\n",
            "Meanwhile, scraped sites often experience customer and revenue losses.Content scraping comprises large-scale content theft from a given site.\n",
            "The two most common use cases are price scraping and content theft.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_sVYsdLQKnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Py94L_kEQKqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9IxgrapQKtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6MjwMZxrQKwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTglaDy6QKy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0V-Ktp5QK1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqfclWQKQK5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoospBAgQLFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.probability import FreqDist\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for non-OK responses\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Content filtering: remove unwanted sections such as scripts, styles, and meta tags\n",
        "        for unwanted_tag in soup([\"script\", \"style\", \"meta\"]):\n",
        "            unwanted_tag.extract()\n",
        "\n",
        "        # Get text from remaining paragraphs\n",
        "        text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "        return text.strip()  # Strip leading and trailing whitespace\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching content from URL: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_summary(text, summary_length=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_frequencies = FreqDist(word_tokenize(text.lower()))\n",
        "    most_frequent_words = [word for word, _ in word_frequencies.most_common(10)]\n",
        "    summary = [sentence for sentence in sentences if any(word in sentence.lower() for word in most_frequent_words)]\n",
        "    return ' '.join(summary[:summary_length])\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        synonyms.update(lemma.name() for lemma in syn.lemmas())\n",
        "    return synonyms\n",
        "\n",
        "def rephrase_sentence(sentence):\n",
        "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]\n",
        "    synonyms = [get_synonyms(word) or {word} for word in words]\n",
        "    rephrased_sentence = [random.choice(list(synonym_set)) for synonym_set in synonyms]\n",
        "    return ' '.join(rephrased_sentence)\n",
        "\n",
        "def rephrase_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    return ' '.join(rephrase_sentence(sentence) for sentence in sentences)\n",
        "\n",
        "def main():\n",
        "    url = input(\"Enter the URL: \")\n",
        "    original_text = get_text_from_url(url)\n",
        "    if original_text:\n",
        "        summary_length = int(input(\"Enter the desired summary length (number of sentences): \"))\n",
        "        original_summary = get_summary(original_text, summary_length)\n",
        "        print(\"\\nOriginal Summary:\")\n",
        "        print(original_summary)\n",
        "        rephrased_summary = rephrase_summary(original_summary)\n",
        "        print(\"\\nRephrased Summary:\")\n",
        "        print(rephrased_summary)\n",
        "    else:\n",
        "        print(\"Error fetching content from URL.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5n31weGkCsB",
        "outputId": "4c27a74c-271d-4b76-f06d-0c52e43fad6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the URL: https://en.wikipedia.org/wiki/Natural_language_processing\n",
            "Enter the desired summary length (number of sentences): 2\n",
            "\n",
            "Original Summary:\n",
            "Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language.\n",
            "\n",
            "Rephrased Summary:\n",
            "raw words work NLP interdisciplinary subfield information_processing_system skill information recovery chiefly have-to_doe_with open computer ability plunk_for fake man words\n"
          ]
        }
      ]
    }
  ]
}